{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNi7iJfwqFpxcKjTjsQ7Ms2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohamed-Qadar/image/blob/main/image_classificationsCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6N_wzPXPF1i",
        "outputId": "dbbcdc15-79e2-435c-a200-f91346cd0e23"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## image classifications using CNN Method"
      ],
      "metadata": {
        "id": "zJpS4iJvzu2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import os\n",
        "\n",
        "# Google Drive'ı bağlayın\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Initialising the CNN\n",
        "classifier = Sequential()\n",
        "\n",
        "# Step 1 - Convolution\n",
        "classifier.add(Convolution2D(32, (3, 3), input_shape=(64, 64, 3), activation='relu'))\n",
        "\n",
        "# Step 2 - Pooling\n",
        "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Adding a second convolutional layer\n",
        "classifier.add(Convolution2D(32, (3, 3), activation='relu'))\n",
        "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Step 3 - Flattening\n",
        "classifier.add(Flatten())\n",
        "\n",
        "# Step 4 - Full connection\n",
        "classifier.add(Dense(units=128, activation='relu'))\n",
        "classifier.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Compiling the CNN\n",
        "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Fitting the CNN to the images\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Veri setlerini yükleme\n",
        "training_set = train_datagen.flow_from_directory('/content/drive/MyDrive/imageClassificationsCNN/training_set',\n",
        "                                                 target_size=(64, 64),\n",
        "                                                 batch_size=32,\n",
        "                                                 class_mode='binary')\n",
        "\n",
        "test_set = test_datagen.flow_from_directory('/content/drive/MyDrive/imageClassificationsCNN/test_set',\n",
        "                                            target_size=(64, 64),\n",
        "                                            batch_size=32,\n",
        "                                            class_mode='binary')\n",
        "\n",
        "# Modeli eğitme\n",
        "classifier.fit(training_set,\n",
        "                steps_per_epoch=len(training_set),\n",
        "                epochs=3,  # epochs = 25,\n",
        "                validation_data=test_set,\n",
        "                validation_steps=len(test_set))\n",
        "\n",
        "# Test görüntüsünü yükleme ve tahmin yapma\n",
        "test_image = image.load_img('/content/drive/MyDrive/imageClassificationsCNN/single_prediction/cat_or_dog_1.jpg', target_size=(64, 64))\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis=0)\n",
        "\n",
        "result = classifier.predict(test_image)\n",
        "\n",
        "# Tahmin sonuçlarını alma\n",
        "print(training_set.class_indices)\n",
        "\n",
        "if result[0][0] == 1:\n",
        "    prediction = 'dog'\n",
        "else:\n",
        "    prediction = 'cat'\n",
        "print(prediction)\n",
        "\n",
        "# İkinci test görüntüsünü yükleme ve tahmin yapma\n",
        "test_image = image.load_img('/content/drive/MyDrive/imageClassificationsCNN/single_prediction/cat_or_dog_2.jpg', target_size=(64, 64))\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis=0)\n",
        "result = classifier.predict(test_image)\n",
        "\n",
        "if result[0][0] == 1:\n",
        "    prediction = 'dog'\n",
        "else:\n",
        "    prediction = 'cat'\n",
        "print(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wP1osb47Vppw",
        "outputId": "792f99ea-5e64-4ccc-98cd-d1f858401881"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8010 images belonging to 2 classes.\n",
            "Found 2000 images belonging to 2 classes.\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2610s\u001b[0m 10s/step - accuracy: 0.5591 - loss: 0.6856 - val_accuracy: 0.6805 - val_loss: 0.6193\n",
            "Epoch 2/3\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 216ms/step - accuracy: 0.6699 - loss: 0.6051 - val_accuracy: 0.7210 - val_loss: 0.5566\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 446ms/step\n",
            "{'cats': 0, 'dogs': 1}\n",
            "dog\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "dog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gerekli olan Kutuphaneleri"
      ],
      "metadata": {
        "id": "G6KgtBONy-l0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Convolution2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "import os"
      ],
      "metadata": {
        "id": "npzB3IhSxQXL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelin Tanımlanması"
      ],
      "metadata": {
        "id": "PLpOxmUdypcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modeli Başlatma\n",
        "classifier = Sequential()\n",
        "\n",
        "# İlk Convolutional Katmanı\n",
        "classifier.add(Convolution2D(32, (3, 3), input_shape=(64, 64, 3), activation='relu'))\n",
        "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "classifier.add(Dropout(0.25))\n",
        "\n",
        "# İkinci Convolutional Katmanı\n",
        "classifier.add(Convolution2D(64, (3, 3), activation='relu'))\n",
        "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "classifier.add(Dropout(0.25))\n",
        "\n",
        "# Üçüncü Convolutional Katmanı\n",
        "classifier.add(Convolution2D(128, (3, 3), activation='relu'))\n",
        "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "classifier.add(Dropout(0.25))\n",
        "\n",
        "# Flattening (Düzleştirme)\n",
        "classifier.add(Flatten())\n",
        "\n",
        "# Tam Bağlantı\n",
        "classifier.add(Dense(units=512, activation='relu'))\n",
        "classifier.add(Dropout(0.5))\n",
        "classifier.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Modelin Derlenmesi\n",
        "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "ISGT85_YtgU9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Veri İşleme ve Veri Yükleme"
      ],
      "metadata": {
        "id": "CEynNuXCyf7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Eğitim ve test veri artırma\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Eğitim ve test setlerini yükleme\n",
        "training_set = train_datagen.flow_from_directory('/content/drive/MyDrive/imageClassificationsCNN/training_set',\n",
        "                                                 target_size=(64, 64),\n",
        "                                                 batch_size=32,\n",
        "                                                 class_mode='binary')\n",
        "\n",
        "test_set = test_datagen.flow_from_directory('/content/drive/MyDrive/imageClassificationsCNN/test_set',\n",
        "                                            target_size=(64, 64),\n",
        "                                            batch_size=32,\n",
        "                                            class_mode='binary')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1AXB-zuxiE4",
        "outputId": "93b39cd0-af5b-435a-c6f5-58eff38eca09"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8010 images belonging to 2 classes.\n",
            "Found 2000 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Model eğitimi ve doğrulama verileri"
      ],
      "metadata": {
        "id": "gQCzb8SByQP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Early stopping ile model eğitimi\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "classifier.fit(training_set,\n",
        "               steps_per_epoch=len(training_set),\n",
        "               epochs=25,\n",
        "               validation_data=test_set,\n",
        "               validation_steps=len(test_set),\n",
        "               callbacks=[early_stopping])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_qfq52wxmRt",
        "outputId": "f8c1edeb-4a33-4bfe-c9bd-f8ef0624a0ce"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 243ms/step - accuracy: 0.5220 - loss: 0.7090 - val_accuracy: 0.5930 - val_loss: 0.6704\n",
            "Epoch 2/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 864us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 3/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
            "  current = self.get_monitor_value(logs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 218ms/step - accuracy: 0.6042 - loss: 0.6628 - val_accuracy: 0.5355 - val_loss: 0.7416\n",
            "Epoch 4/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 5/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 220ms/step - accuracy: 0.6542 - loss: 0.6234 - val_accuracy: 0.6985 - val_loss: 0.5780\n",
            "Epoch 6/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 7/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 223ms/step - accuracy: 0.6985 - loss: 0.5794 - val_accuracy: 0.7435 - val_loss: 0.5305\n",
            "Epoch 8/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 9/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 218ms/step - accuracy: 0.7224 - loss: 0.5430 - val_accuracy: 0.7325 - val_loss: 0.5301\n",
            "Epoch 10/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 11/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 222ms/step - accuracy: 0.7370 - loss: 0.5274 - val_accuracy: 0.7465 - val_loss: 0.5014\n",
            "Epoch 12/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 13/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 225ms/step - accuracy: 0.7520 - loss: 0.5124 - val_accuracy: 0.7885 - val_loss: 0.4514\n",
            "Epoch 14/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 15/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 220ms/step - accuracy: 0.7517 - loss: 0.4951 - val_accuracy: 0.7850 - val_loss: 0.4476\n",
            "Epoch 16/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 17/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 218ms/step - accuracy: 0.7633 - loss: 0.4794 - val_accuracy: 0.7895 - val_loss: 0.4521\n",
            "Epoch 18/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 19/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 217ms/step - accuracy: 0.7710 - loss: 0.4731 - val_accuracy: 0.8190 - val_loss: 0.4137\n",
            "Epoch 20/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 21/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 214ms/step - accuracy: 0.7897 - loss: 0.4555 - val_accuracy: 0.8070 - val_loss: 0.4218\n",
            "Epoch 22/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 23/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 217ms/step - accuracy: 0.7979 - loss: 0.4340 - val_accuracy: 0.8240 - val_loss: 0.4003\n",
            "Epoch 24/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 25/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 219ms/step - accuracy: 0.7982 - loss: 0.4303 - val_accuracy: 0.7825 - val_loss: 0.4770\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f046233cb80>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# EarlyStopping callback\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',  # Doğrulama kaybını takip et\n",
        "    patience=5,          # 5 epoch boyunca iyileşme olmazsa eğitimi durdur\n",
        "    restore_best_weights=True  # En iyi ağırlıkları geri yükle\n",
        ")\n",
        "\n",
        "# Eğitim ve doğrulama veri setleri için .repeat() ekleyin\n",
        "training_set = training_set.repeat()  # Eğitim veri setini tekrarla\n",
        "test_set = test_set.repeat()          # Doğrulama veri setini tekrarla\n",
        "\n",
        "# Model eğitimi\n",
        "classifier.fit(\n",
        "    training_set,\n",
        "    steps_per_epoch=len(training_set),  # Eğitim veri kümesindeki adım sayısı\n",
        "    epochs=25,                          # Maksimum epoch sayısı\n",
        "    validation_data=test_set,           # Doğrulama veri seti\n",
        "    validation_steps=len(test_set),     # Doğrulama veri setindeki adım sayısı\n",
        "    callbacks=[early_stopping]          # EarlyStopping callback'i\n",
        ")\n"
      ],
      "metadata": {
        "id": "r5cqun8e-yed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Eğitim sırasında elde edilen verileri alıyoruz\n",
        "history = classifier.fit(training_set,\n",
        "                         steps_per_epoch=len(training_set),\n",
        "                         epochs=25,\n",
        "                         validation_data=test_set,\n",
        "                         validation_steps=len(test_set),\n",
        "                         callbacks=[early_stopping])\n",
        "\n",
        "# Eğitim ve doğrulama kaybını çizme\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(history.history['loss'], label='Eğitim Kaybı')\n",
        "plt.plot(history.history['val_loss'], label='Doğrulama Kaybı')\n",
        "plt.title('Eğitim ve Doğrulama Kaybı')\n",
        "plt.xlabel('Epok Sayısı')\n",
        "plt.ylabel('Kayıp (Loss)')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Eğitim ve doğrulama doğruluğunu çizme\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(history.history['accuracy'], label='Eğitim Doğruluğu')\n",
        "plt.plot(history.history['val_accuracy'], label='Doğrulama Doğruluğu')\n",
        "plt.title('Eğitim ve Doğrulama Doğruluğu')\n",
        "plt.xlabel('Epok Sayısı')\n",
        "plt.ylabel('Doğruluk (Accuracy)')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bNRPAo453L3",
        "outputId": "a259b001-7cd5-4e88-b0ad-5c2bd503a5f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 216ms/step - accuracy: 0.8037 - loss: 0.4158 - val_accuracy: 0.8080 - val_loss: 0.4065\n",
            "Epoch 2/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 3/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 217ms/step - accuracy: 0.7966 - loss: 0.4282 - val_accuracy: 0.7950 - val_loss: 0.4300\n",
            "Epoch 4/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 5/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 216ms/step - accuracy: 0.8218 - loss: 0.3989 - val_accuracy: 0.8375 - val_loss: 0.3609\n",
            "Epoch 6/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 7/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 215ms/step - accuracy: 0.8217 - loss: 0.3867 - val_accuracy: 0.8320 - val_loss: 0.3830\n",
            "Epoch 8/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 9/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 220ms/step - accuracy: 0.8241 - loss: 0.3895 - val_accuracy: 0.8375 - val_loss: 0.3677\n",
            "Epoch 10/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 11/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.8322 - loss: 0.3795"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tahmin Fonksiyonu"
      ],
      "metadata": {
        "id": "KW0iB53RyGf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tahmin fonksiyonu\n",
        "def predict_image(image_path):\n",
        "    test_image = image.load_img(image_path, target_size=(64, 64))\n",
        "    test_image = image.img_to_array(test_image)\n",
        "    test_image = np.expand_dims(test_image, axis=0)\n",
        "    result = classifier.predict(test_image)\n",
        "    return 'dog' if result[0][0] >= 0.5 else 'cat'\n"
      ],
      "metadata": {
        "id": "x5lb6MRZxuun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performans Değerlendirme"
      ],
      "metadata": {
        "id": "qUjDeGSEx6V-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Tahminleri hesaplama\n",
        "y_true = test_set.classes\n",
        "y_pred = classifier.predict(test_set)\n",
        "y_pred = (y_pred > 0.5).astype(int).flatten()\n",
        "\n",
        "# Metriklerin hesaplanması\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "print(f\"Doğruluk (Accuracy): {accuracy:.2f}\")\n",
        "print(f\"Hassasiyet (Precision): {precision:.2f}\")\n",
        "print(f\"Hatırlama (Recall): {recall:.2f}\")\n",
        "print(f\"F1 Skoru: {f1:.2f}\")\n"
      ],
      "metadata": {
        "id": "edouP-bSxxdI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}